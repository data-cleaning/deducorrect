%\VignetteIndexEntry{deducorrect-imputation}
\documentclass[11pt, fleqn, a4paper]{article}
\usepackage[english]{babel}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations,calc}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{amsmath, amssymb}
\usepackage{natbib}
\usepackage{algpseudocode}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{threeparttable}

% stimulate latex to put multiple floats on a page.
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{3}
\setcounter{dbltopnumber}{2}
\renewcommand{\topfraction}{.9}
\renewcommand{\textfraction}{.1}
\renewcommand{\bottomfraction}{.75}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\dblfloatpagefraction}{.9}
\renewcommand{\dbltopfraction}{.9}
\hyphenation{time-stamp}

\usepackage{float}
 
\floatstyle{boxed}
\newfloat{Rcode}{t}{rco}
\floatname{Rcode}{Figure}


\title{Deductive imputation with the {\sf deducorrect} package}
\author{Mark van der Loo and Edwin de Jonge\\
{\small Package version \Sexpr{packageVersion("deducorrect")}}}
\begin{document}
\maketitle
\begin{abstract}



\end{abstract}

<<echo=FALSE,results=hide, keep.source=FALSE>>=
library(editrules)
library(deducorrect)
@
\maketitle

\newpage

\tableofcontents

\newpage
\section{Introduction}
The quality of raw survey data is only rarely sufficient to allow for immediate
statistical analysis. The presence of missing values (nonresponse) and
inconsistencies impedes straightforward application of standard statistical
estimation methods, and statisticians often have to spend considerable effort
to counterbalance the effect of such errors. 

There are basically two ways to take the effect of data quality issues into
account.  The first is to adapt the statistical analysis such that the effects
of these issues are taken into account. One well-documented example is to use
weighting methods which take the effect of (selective) item nonresponse into
account {\bf ref } \citep{bethlehem:2011}.  The second way is to clean up the
dataset so that missing values are completed and inconsistencies have been
repaired. The latter method has the advantage that statistical analyses of the
data becomes to a degree independent of the models used in data cleaning.
Whichever way is chosen, in most cases additional assumtions are necessary to
clean data or interpret the results of data analyses. 

Recently, a number of of near assumption-free data-cleaning methods have been
reported which rely almost purely on record consistency rules imposed {em a
priori} the data. Examples of such rules include account balances, positivity
demands on variables or forbidden value combinations in categorical data.  In a
previous paper \citep{loo:2011a} we reported on methods which use data
consistency rules and information in inconsistent records to track down and
repair typing errors, rounding errors and sign errors. The theory behind these
methods was first published by \citet{scholtus:2008, scholtus:2009} and were
implemented by us in {\sf R} package {\sf deducorrect}. Since these so-called
deductive correction methods are based on adapting values, they are not suited
for completing missing values.

In this paper, we report on an extension of the {\sf deducorrect} package which
allows for deductive imputation of missing values in either numerical or
categorical data.  By deductive imputation we mean methods which use the
observed values in a record together with consistency rules imposed on the
record to uniquely derive values where possible.  The values may be missing
because of nonresponse, or they may be deemed missing by an error localization
algorithm such as implemented in the {\sf editrules} package
\citep{jonge:2011a, loo:2011c}. 

In section \ref{sdeduimpute}, we further introduce the concept of deductive
correction and show the easyest way of imputing values with the deducorrect
package. In sections \ref{sdeductivenumerical} and \ref{sdeductivecategorical}
we expand a bit on the theory and demonstrate the use of lower-level
functionality of the package. Examples in {\sf R} code are given throughout to
help new users getting started.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deductive imputation}
\label{sdeduimpute}

\subsection{Overview}
Deductive imputation relies on in-record consistency rules to derive the value
of variables which have not been completed from variables which have been
completed. These methods therefore rely on the assumption that the values used
in the derivation have been completed correctly. 
For example, suppose we have a numerical record ${\bf x}=(x_1,x_2,x_3)$, subject to the
rules
\begin{eqnarray}
\label{rule1}
x_1 + x_2 &=& x_3\\
\label{rule2}
{\bf x} &\geq& \boldsymbol{0}. 
\end{eqnarray}
Suppose we are given two values of ${\bf x}$, for example $({\sf NA},x_2,x_3)$,
where {\sf NA} stands for Not Available. In principle, the third value is
easily derived from rule \eqref{rule1}. However, if either for example $x_2<0$,
the derived value for $x_1$ is most likely not the true value, since at least
one of the values used to derive $x_1$ is invalid. Moreover, if $x_2>x_3$, the
derived value for $x_1$ will be negative, and therefore violate rule
\eqref{rule2}. For categorical data, analogous situations may arise.

The deductive imputation routines of the {\sf deducorrect} package offer two
mechanisms to avoid inconsitencies. The first is to explicitly check if
consistent deductive imputation is possible based on the observed values.  This
is turned on by default for the functions {\sf deduImpute}, {\sf
deductiveZeros}, the {\sf editmatrix} method of {\sf solSpace} and {\sf
deductiveLevels}. All these functions which will be discussed in the coming
sections. The second mechanism is the ability to point out variables besides
the missing ones, which should be considered as if they are missing. A typical
example would be to use the result of an error localization algorithm which
points out erroneous fields in a record.


\begin{figure}[!t]
% tikz style definitions
    \tikzstyle{decision} = [diamond, draw, fill=blue!20, 
        text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
    \tikzstyle{block} = [rectangle, draw, fill=blue!20, 
        text width=5em, text centered, rounded corners, minimum height=3em]
    \tikzstyle{line} = [draw, -latex']
    \tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
        text width=9em, minimum height=2em, align=center]
\begin{tikzpicture}[node distance = 2cm, auto, decoration={brace,amplitude=10pt}]
    % Flow diagram blocks
    \node [block] (init0) {read data};
    \node [block, below of=init0]                       (init1)     {read rules};
    \node [block, below of=init1]                       (deduimp1)  {deductive imputation};
    \node [block, below of=deduimp1]                    (deducorr)  {deductive correction};
    \node [decision, below of=deducorr]                 (decide)    {data has changed};
    \node [block, below of=decide, node distance=3cm]   (errloc)    {error localization};
    \node [block, below of=errloc]                      (deduimp2)  {deductive imputation};
    \node [block, below of=deduimp2]                    (end)       {other imputation methods};
    % draw lines
    \path [line] (init0)   -- (init1);
    \path [line] (init1)   -- (deduimp1);
    \path [line] (deduimp1)-- (deducorr);
    \path [line] (deducorr)-- (decide);
    \path [line] (decide)  -- node {no} (errloc);
    \path [line] (errloc)  -- (deduimp2);
    \path [line] (deduimp2)  -- (end);
    % looping path
    \path [line] (decide.west) 
    % go left
        -- ++(-.7,0) node[below] {yes}  
    % up, and to deduimp.west.
        |- (deduimp1.west);
    
    % functionality blocks
    \node [cloud, right of=init1, node distance=4cm] (editrules) {{\sf editarray}, {\sf editmatrix}};
    \node [cloud, right of=deduimp1, node distance=4cm] (ddimp1) {{\sf deduImpute}};
    \node [cloud, right of=deducorr, node distance=4cm] (ddcor1) {{\sf correctTypos} {\sf correctSigns} {\sf correctRounding}};
    \node [cloud, right of=errloc, node distance=4cm] (ddcor2) {{\sf localizeErrors}};
    \node [cloud, right of=deduimp2, node distance=4cm] (ddimp2) {{\sf deduImpute}};
    % package annotation
    \draw [decorate, line width=1pt] ($(init1.north) + (7,0)$) -- ($(init1.south) + (7,0)$)
       node [midway, anchor=west, outer sep=3ex]{{\sf editrules}};
    \draw [decorate, line width=1pt] ($(deduimp1.north) + (7,0)$) -- ($(deducorr.south) + (7,0)$)
       node [midway, anchor=west, outer sep=3ex]{{\sf deducorrect}};
    \draw [decorate, line width=1pt] ($(errloc.north) + (7,0)$) -- ($(errloc.south) + (7,0)$)
       node [midway, anchor=west, outer sep=3ex]{{\sf editrules}};
    \draw [decorate, line width=1pt] ($(deduimp2.north) + (7,0)$) -- ($(deduimp2.south) + (7,0)$)
       node [midway, anchor=west, outer sep=3ex]{{\sf deducorrect}};
    \end{tikzpicture}

\caption{Flow diagram showing how functionality of the {\sf deducorrect} and
    {\sf editrules} can be combined to perform the deductive corrections, deductive
    imputations and error localization. All steps except deductive correction are
    available for numerical as well as categorical data. The ellipses indicate some
    of the {\sf R} functions from the packages noted on the right. }
\label{flowchart}
\end{figure}

In the context of a complete automated data editing system, there are a number
of places where deductive imputation can be applied. Typically, one will apply
such methods before the data is treated with more complicated imputation
models. Figure \ref{flowchart} shows a general flowchart for the first step in
automated data cleaning. After these steps are performed, all (near)
assumption-free corrections offered by {\sf deducorrect} have been performed.
For further imputations and corrections one has to resort to other methods,
making new model assumptions. It should be noted that a common step such as
detecting and repairing unit measure errors is not included here. However, such
methods are easily implemented in {\sf R}, and we refer to \cite{waal:2011} for
an overview. 

Deductive imputation appears twice in the the process flow chart of Figure
\ref{flowchart}.  The reason is that in the presence of missing data, it is
possible that not all rules can be checked. For this reason the process stats
with deductively imputing as many values as possible.  After this deductive
corrections, can be applied which in turn can yield a higher data quality,
opening up the possibility for more deductive imputations. This process can be
iterated untill no data quality is gained anymore. After this, the smallest
(weighted) number of variables to adapt or impute can be determined using error
localization functionality of the {\sf editrules} package. The resulting error
locations can serve as extra input to another run of the {\sf deduImpute}
function. After these steps have been performed, no imputations based on the
rules and observed values can be derived anymore.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Imputation with {\sf deduImpute}}
The simplest way to do deductive imputations with the {\sf deducorrect} package
is to use the {\sf deduImpute} function. It can be used for both numerical and
categorical data. The function accepts an {\sf editmatrix} or {\sf editarray}
containing the editrules and a {\sf data.frame} containing the records. The
return value is an object of class {\sf deducorrect}, similar to the values
returned by the {\sf correct-} functions of deducorrect [see
\citet{loo:2011a}].

For numerical data it uses two methods (described sections \ref{sssolspace} and
\ref{ssdeductivezeros}) to impute as many empty values as possible.  It uses
the functions {\sf solSpace} and {\sf deductiveZeros} iteratively for each
record untill no deductive improvements can be made. Here, we will use the
example from \citet{waal:2011}, Chapter 9.2. This example uses the following
edits, based on a part of the Dutch Structural Business Survey balance account.
\begin{equation}
\begin{array}{rcl}
    x_1 + x_2      &=& x_3\\
    x_2           &=& x_4\\
    x_5 + x_6 + x_7 &=& x_8\\
    x_3 + x_8      &=& x_9\\
    x_9 - x_{10}     &=& x_{11}\\
    x_6 &\geq& 0\\
    x_7 &\geq& 0
\end{array}
\label{eqdeduImputeEdits}
\end{equation}
The rule $x_2=x_4$ may seem odd for readers not familiar to survey statistics.
However, these rules correspond to cases where respondents have to copy a
figure from one page on a paper form to another\footnote{In spite of the
availability of web-based forms, many respondents prefer paper forms.}.  
%
%
\begin{Rcode}
<<keep.source=TRUE>>=
E <- editmatrix(c(
         "x1 + x2      == x3",
         "x2           == x4",
         "x5 + x6 + x7 == x8",
         "x3 + x8      == x9",
         "x9 - x10     == x11",
         "x6 >= 0",
         "x7 >= 0"
))
dat <- data.frame(
    x1=c(145,145),
    x2=c(NA,NA),
    x3=c(155,155),
    x4=c(NA,NA),
    x5=c(NA, 86),
    x6=c(NA,NA),
    x7=c(NA,NA),
    x8=c(86,86),
    x9=c(NA,NA),
    x10=c(217,217),
    x11=c(NA,NA)
)
dat
d <- deduImpute(E,dat)
d$corrected
@
\caption{A simple example with {\sf deduImpute}. The return value is an object of class {\sf deducorrect}.}
\label{RdeduImpute}
\end{Rcode}
%
%
In Figure \ref{RdeduImpute} we give an example where the following record subject
to the edits in Eq.\ \eqref{eqdeduImputeEdits} is treated.
%
<<keep.source=FALSE,echo=FALSE>>=
dat[1,]
@
%
The record contains missing values. However, by assuming that all non-missing
values are correct, values can be derived for $x_2$, $x_4$, $x_9$ and $x_{11}$
just by considering the equality- and nonnegativity rules in the edit set.

The assumption that all missing values can be imputed consistently may not
alway be valid: the nonmissing values may be filled in erroneously, yielding
faulty derived values to impute. The reason is that {\sf deduImpute} does not
take into account all edit rules: only nonnnegativity rules and equality rules
are used to derive imputed values.

The {\sf deduImpute} function has two mechanisms to get around this. The first
is to set the option {\sf checkFeasibility=TRUE}. This causes solutions causing
new inconsistencies to be rejected. The second mechanism is to provide a
user-specified {\sf adapt} array to increase the number of variables which may
be imputed, missing or not. The {\sf adapt} array is a boolean array, stating
which variable may be changed in which record. A convenient example is to use
the {\sf adapt} array as generated by the {\sf localizeErrors} function from
the {\sf editrules} package. By specifying an {\sf adapt} array, {\sf
deduImpute} will try to fix records by imputing values which are either missing
or may be adapted according to {\sf adapt}.


For categorical data, {\sf deduImpute} uses the {\sf deductiveLevels} function,
discussed in section \ref{sdeductivecategorical}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DEDUCTIVE IMPUTATION OF NUMERICAL DATA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deductive imputation of numerical data}
\label{sdeductivenumerical}

The valid value combinations of numerical data records with $n$ variables are
usually limited to some subset of $\mathbb{R}^n$. Common cases include balance
accounts (linear restrictions) combined with linear inequality rules
(positivity rules for example). In such cases the set of valid records is a
convex polytope or polyhedral cone. In certain cases, when the values for a
number of variables have been fixed, the set of possible values for a number of
the remaining variables reduces to a point. In such cases deductive imputation
is posible.

\subsection{Imputation with {\sf solSpace} and {\sf imputess}}
\label{sssolspace}
\subsubsection{Area of application}
The combination of functions {\sf solSpace} and {\sf imputess} can be used
to impute numerical data under linear equality restrictions:
\begin{equation}
{\bf Ax} = {\bf b}\textrm{, with } {\bf A}\in\mathbb{R}^{m\times n},
\quad{\bf x}\in\mathbb{R}^n\textrm{ and }
{\bf b}\in\mathbb{R}^m.
\label{solspaceImputation}
\end{equation}
If {\bf x} has missing values, then {\sf solSpace} returns a representation
of the linear space of imputations valid under Eqn.\
\eqref{solspaceImputation}.  The function {\sf imputess} performs the actual
imputation. It is important to note that these functions do not take into
account the presence of any inequality restrictions.

\subsubsection{How it works}
\label{ssssolSpace}
Consider a numerical record $\bf x$ with $n_{miss}$ values missing.  
The values may be missing because of nonresponse, or they may be 
deemed missing by an error localization procedure (see the next subsection).
We will
write ${\bf x}=({\bf x}_{\rm obs},{\bf x}_{\rm miss})$, with ${\bf x}_{\rm
obs}$ the observed values and ${\bf x}_{\rm miss}$ the missing ones.  Supposing
further that $\bf x$ must obey a set of equality restrictions as in Eqn.
\eqref{solspaceImputation}, we may write
${\bf A}=[{\bf A}_{\rm obs},{\bf A}_{\rm miss}]$. Consequently we have \citep{waal:2011}
\begin{equation}
    {\bf A}_{\rm miss}{\bf x}_{\bf miss} = {\bf b} - {\bf A}_{\rm obs}{\bf x}_{obs}.
\end{equation}
This gives
\begin{equation}
    {\bf x}_{\rm miss} = {\bf x}_0 + {\bf C}{\bf z},
\end{equation}
with {\bf z} an arbitrary real vector of dimension $n_{\rm miss}$ and 
${\bf x}_0$ and {\bf C} constant.

The purpose of {\sf solSpace} is to compute ${\bf x}_0$ and ${\bf C}$.
Together they determine the vector space of values available for ${\bf x}_{\sf
miss}$.  Deductive imputation can be realized by observing that if any rows of
${\bf C}$ are filled with zeros, then the sole value for the corresponding
values of ${\bf x}_{\sf miss}$ are given the corresponding values in ${\bf
x}_0$. The values of ${\bf x}_0$ and ${\bf C}$ are given by
\begin{eqnarray}
    {\bf x}_0 &=& {\bf A}^+_{\rm miss}({\bf b}-{\bf A}_{\rm obs}{\bf x}_{\rm obs})\\
    {\bf C} &=& {\bf A}^+_{\rm miss}{\bf A}_{\rm miss} - \boldsymbol{1}.
\end{eqnarray}
Here, $\boldsymbol{1}$ is the identity matrix and ${\bf A}^+_{\rm miss}$ is the
generalized inverse of {\bf A}, obeying
\begin{equation}
{\bf A}_{\rm miss}{\bf A}^+_{\rm miss}{\bf A}_{\rm miss}={\bf A}_{\rm miss}.
\end{equation}
See \citet{waal:2011} for details on the imputation method or \citet{greville:1959} for
an excellent discussion on the pseudoinverse.

\subsubsection{An example}
The {\sf solSpace} function returns the ${\bf x}_0$ and ${\bf C}$ as a
list. For example consider the first record from Figure \ref{RdeduImpute}:
<<echo=TRUE>>=
(x <- dat[1,])
@
Using the editmatrix defined in the same figure, we get:
<<keep.source=TRUE>>=
(s <- solSpace(E,x))
@
{\sf solSpace} has an extra argument {\sf adapt} which allows extra fields of
${\bf x}$ to be considered missing. An example of its use would be to determine
erroneous fields with {\sf errorLocalizer} (of the {\sf editrules} package) and
to determine the imputation space with {\sf solSpace}.

The top two and bottom two rows of {\bf C} in the example have zero
coefficients, yielding a unique solution for $x_2$, $x_3$, $x_9$ and $x_{11}$.
The unique values may be imputed with {\sf imputess}:
<<keep.source=TRUE>>=
imputess(x, s$x0, s$C)
@
If a ${\bf z}$-vector is provided as well, all values may be imputed. Here, we
choose ${\bf z}=\boldsymbol{0}$ (arbitrarily).
<<keep.source=TRUE>>=
( y <- imputess(x, s$x0, s$C, z=rep(0,ncol(s$C))) )
@
Using {\sf violatedEdits} from the editrules package, we may verify that this record satisfies every inequality
rule as well ({\sf E} as in figure \ref{RdeduImpute}).
<<keep.source=TRUE>>=
any(violatedEdits(E,y,tol=1e-8))
@

To demonstrate the use of the {\sf adapt} argument, consider the following case.
<<>>=
Ey <- editmatrix(c(
    "yt == y1 + y2 + y3",
    "y4 == 0"))
y <- c(yt=10, y1=NA, y2=3, y3=7,y4=12)
@
%Here, $y_4$ clearly violates the second rule. (A rule like this may arise when manipulating
%edit sets or pre-substituting a number of values). Since $y_4$ is not empty, {\sf solSpace}
%ignores it unless told otherwise.
<<>>=
(s <- solSpace(Ey,y))
#imputess(y,x0=s$x0,C=s$C)
@
However, using the {\sf adapt} argument, which is a logical indicator stating which
entries may be adapted, we get the following.
<<>>=
(s <- solSpace(Ey, y, adapt=c(FALSE,FALSE,FALSE,FALSE,TRUE)))
imputess(y,x0=s$x0,C=s$C)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Imputation with {\sf deductiveZeros}}
\label{ssdeductivezeros}
\subsubsection{Area of application}
This method can be used to impute missing values in numerical records
subject to
\begin{eqnarray}
{\bf Ax} &=& {\bf b}\textrm{, with } {\bf A}\in\mathbb{R}^{m\times n},
\quad{\bf x}\in\mathbb{R}^n\textrm{ and }
{\bf b}\in\mathbb{R}^m
\\
{x}_j &\geq& 0 \textrm{ for at least one }j\in\{1,2,\ldots,n\}.
\label{deductiveZeroImputation}
\end{eqnarray}
Economic  survey data are often subject to account balances of the $x_t = x_1 +
x_2+\cdots x_k$. For example, $x_t$ might be the total personell cost and the
$x_i$ are costs related to permanent staff, temporary staff, externals, {\em
etc.}. It is not uncommon for respondents to leave fields open which are not
relevant to them. For example, if a company has not hired any temorary staff,
the corresponding field might be left empty while a 0 would have been
appropriate.  

In such cases, missing values are bounded from above by the sum rules while
they are bounded from below by the nonnegativity constraint. If the missing
values are ignored, and the completed values add up to the required totals,
then missing values may be uniquely imputed with 0. The function {\sf
deductiveZeros} detects such cases.

\subsubsection{How it works}
Consider again the notation of Section \ref{ssssolSpace}. We write (following
notation of \cite{waal:2011}).
\begin{equation}
{\bf b}^* = {\bf b} - {\bf A}_{\rm obs}{\bf x}_{\rm obs}.
\end{equation}
If any $b^*_l=0$, this means that the sum rule ${\bf a}_l\cdot {\bf x}=b_l$ is
obeyed if missing values are ignored. For those cases, the following properties are
checked.
\begin{itemize}
\item Each $a_{{\rm miss},lj\not=}$ has the same sign.
\item Each $a_{{\rm miss},lj}\not=0$ corresponds to a variable $x_j$ that is constrained to be nonnegative.
\end{itemize}
If these demands are obeyed, the corresponding value $x_{{\rm miss},j}$ may be imputed with 0.

\subsubsection{An example}
The function {\sf deductiveZeros} does not perform imputation itself but computes
an indicator stating which values may be imputed. As a first example consider
the following.
<<keep.source=TRUE>>=
Ey <- editmatrix(c(
    "yt == y1 + y2 + y3",
    "y1 >= 0", "y2 >= 0 ","y3 >= 0"))
y <- c(yt=10, y1=NA, y2=3, y3=7)
(I<-deductiveZeros(Ey,y))
@
The record $y$ can be imputed in one statement.
<<keep.source=TRUE>>=
y[I] <- 0
y
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DEDUCTIVE IMPUTATION OF CATEGORICAL DATA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deductive Imputation of categorical data}
\label{sdeductivecategorical}

A categorical data records is a member of the cartesian product
\begin{equation}
D = D_1\times D_2\times\cdots\times D_n,
\label{eqdomain}
\end{equation}
where each $D_k$ is the set of categories for a variable. In practice
not every record in $D$ may be acceptable. For example if 
\begin{equation}
D = \{\textrm{child},{\rm adult}\}\times \{{\rm married},{\rm unmarried}\},
\end{equation}
then the record $(\textrm{child},{\rm married})$ may be excluded from the set
of valid records. Therefore, if we have a record with $({\sf NA},{\rm
married})$, and assume that the marital status is correct, there is only one
possible value for the age class, namely ``adult''. So just like for numerical
data, if the known values limit the number of options for the unknowns to a
unique value, deductive imputation is possible.


\subsection{Deductive imputation with {\sf deductiveLevels}}
\subsection{Area of application}
The function {\sf deductiveLevels} works on purely categorical data where the
number of categories for each variable is known and fixed, as in Eq.\
\eqref{eqdomain}.  It determines which missing values in a record are
determined uniquely by the known values, and these unique values are returned.

\subsubsection{How it works}
The algorithm which derives possible values for the missing variables is based
on substitution of known values in the set of edits and elimination of 

\begin{algorithmic}
\Require {An {\sf editarray} $E$, a partially complete record $({\bf v}_{\sf obs},{\bf v}_{\sf mis})$}
\State Determine the index $I\subset\{1,2,\ldots n\}$ in ${\bf v}$ of observed values.
\State $E\leftarrow${\sc substValue($E$,$I$,${\bf v}_{I}$)}
\If {$\lnot\textrm{\sc isFeasible}(E)$}
\State \Return {\sc null}
\EndIf
\State $M\leftarrow \{1,2,\ldots,n\}\backslash I$.
\State $m\leftarrow |M|$
\State $T\leftarrow \varnothing$
\For {$i \in \{1,2,\ldots,m\}$}
\State haha
\EndFor
\end{algorithmic}

\subsubsection{Examples}



\section{Conclusions}

\bibliographystyle{chicago}
\bibliography{deducorrect}
\end{document}
